# graspcg/regularization/tv_nd.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Mapping, Optional, Sequence, Tuple, List, Union

import torch

from .base import Regularizer, RegParams, RegContext, AxesSpec
from ..core.roles import Roles

# Use your forward/backward difference stencils
from ..ops.ndops import fwd_diff, bwd_diff, tv_aniso_div,tv_aniso_flux, tv_aniso_energy,tv_iso_div,tv_iso_energy,tv_iso_flux   # compile-friendly TV stencils

# ---------------------------
# Helpers (no Python floats)
# ---------------------------

def _abs2(x: torch.Tensor) -> torch.Tensor:
    # |x|^2 in REAL dtype regardless of complex/real input
    return (x.conj() * x).real

def _as0(x: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:
    return torch.zeros((), device=x.device, dtype=dtype)

def _resolve_axes_local(spec: AxesSpec, roles: Roles) -> Tuple[int, ...]:
    u, l, n = int(roles.unlike), int(roles.like), int(roles.nufft)
    total = u + l + n

    def rng(s, c): return list(range(s, s+c))
    tok = {
        "temporal": rng(0, u), "time": rng(0,u), "unlike": rng(0,u),
        "like": rng(u, l),
        "spatial": rng(u+l, n), "nufft": rng(u+l, n), "image": rng(u+l, n),
    }
    out: List[int] = []
    if isinstance(spec, str):
        out.extend(tok.get(spec.lower(), []))
    elif isinstance(spec, (list, tuple)):
        for s in spec:
            if isinstance(s, str):
                out.extend(tok.get(s.lower(), []))
            elif isinstance(s, int):
                ax = s if s >= 0 else (total + s)
                if ax < 0 or ax >= total:
                    raise ValueError(f"Axis index {s} out of range for dims={total}")
                out.append(ax)
            else:
                raise TypeError(f"Bad axis element: {type(s)}")
    else:
        raise TypeError(f"Bad axes spec type: {type(spec)}")
    # dedup preserve order
    seen, uniq = set(), []
    for a in out:
        if a not in seen:
            seen.add(a); uniq.append(a)
    return tuple(uniq)

# ---------------------------
# Parameters
# ---------------------------

@dataclass(frozen=True)
class TVParams(RegParams):
    """
    N‑D Total Variation parameters.

    • weight: λ
    • eps   : Huber knee (smoothing)
    • axes  : which axes; tokens or indices
    • isotropic    : True ⇒ ℓ2 coupling across chosen axes, False ⇒ ℓ1 across axes
    • axis_weights : Optional per-axis scalars (same length/order as 'axes') to
                     realize anisotropic dimensions (e.g., voxel spacing or axis gains).
                     If None, all ones.
    """
    weight: float = 0.0
    eps: float = 1e-3
    axes: AxesSpec = "spatial"
    isotropic: bool = True
    axis_weights: Optional[Sequence[float]] = None


class TVND(Regularizer):
    """
    N‑D TV with optional per‑axis weights and optional per‑batch scaling TV(x/s).

    Contract:
      • energy_grad[_fixed_axes] returns a 0‑D REAL tensor (device==ctx.device)
      • accumulates gradient into ctx.g in-place
      • uses ctx.write_interior_slice to confine writes when halo is present

    Scaling semantics:
      If params.use_scale=True and ctx.scale_field_shard is provided ((B_loc,1,...,1) == 1/s),
      the kernel computes TV(u) with u = x/s exactly on the discrete grid:
        1) form u = x * (1/s) (padded across halo along the sharded axis),
        2) take forward differences on u,
        3) compute energy density / flux / divergence as usual,
        4) apply chain rule to the gradient: ∂E/∂x = (∂E/∂u) * (1/s).
      ε is kept as a constant smoothing knee in the same (real) dtype.
    """

    Params = TVParams

    def __init__(self, name: str, params: TVParams):
        self.name = name
        self.params = params

    # ---- required API ----
    def energy_grad(self, ctx: RegContext) -> torch.Tensor:
        axes = self._resolve_axes(ctx)
        return self.energy_grad_fixed_axes(ctx, axes)

    def energy_grad_fixed_axes(self, ctx: RegContext, axes: Tuple[int, ...]) -> torch.Tensor:
        dev, dr = ctx.device, ctx.dtype_r
        if len(axes) == 0 or self.params.weight == 0.0:
            return torch.zeros((), device=dev, dtype=dr)

        # per-axis weights (anisotropic dimensions)
        if self.params.axis_weights is not None:
            if len(self.params.axis_weights) != len(axes):
                raise ValueError(f"axis_weights must match len(axes). "
                                f"Got {len(self.params.axis_weights)} vs {len(axes)}")
            w_axes = [torch.tensor(float(w), device=dev, dtype=dr) for w in self.params.axis_weights]
        else:
            w_axes = [torch.tensor(1.0, device=dev, dtype=dr) for _ in axes]

        x_ext = ctx.x
        interior = ctx.write_interior_slice or (slice(None),) * x_ext.ndim

        # Build halo-extended (1/s) if enabled; else a broadcastable 1
        use_scale = bool(getattr(self.params, "use_scale", False)) and (ctx.scale_field_shard is not None)
        if use_scale:
            s_int = ctx.scale_field_shard  # (B_loc,1,...,1)
            B_ext = int(x_ext.shape[0])
            start = int(interior[0].start or 0)
            stop  = int(interior[0].stop  or B_ext)
            left  = start
            right = B_ext - stop
            if left > 0 or right > 0:
                s_ext = torch.cat([
                    s_int[0:1].expand(left,  *s_int.shape[1:]),
                    s_int,
                    s_int[-1:].expand(right, *s_int.shape[1:]),
                ], dim=0)
            else:
                s_ext = s_int
            s_ext = s_ext.contiguous()  # (B_ext,1,...,1) == 1/s
        else:
            s_ext = torch.ones((1, *([1] * (x_ext.ndim - 1))), device=dev, dtype=dr)

        # u = x/s, do all TV ops on u
        x_eff = x_ext * s_ext

        # forward diffs on u, apply per-axis weights
        grads = [wi * fwd_diff(x_eff, ax) for wi, ax in zip(w_axes, axes)]

        # parameters as REAL tensors
        w = torch.tensor(self.params.weight, device=dev, dtype=dr)
        eps = torch.tensor(self.params.eps,    device=dev, dtype=dr)

        # energy density and divergence
        if self.params.isotropic:
            e_den_full = tv_iso_energy(grads, eps)    # REAL
            flux = tv_iso_flux(grads, eps)
            div_p_full = tv_iso_div(flux, axes)       # same shape as x
        else:
            e_den_full = tv_aniso_energy(grads, eps)  # REAL
            flux = tv_aniso_flux(grads, eps)
            div_p_full = tv_aniso_div(flux, axes)

        # restrict to interior (drop halo on writes)
        e_den = e_den_full[interior] if interior is not None else e_den_full
        div_p = div_p_full[interior] if interior is not None else div_p_full

        # chain rule: ∂E/∂x = (∂E/∂u) * (1/s)  (use interior slice of s_ext, if present)
        s_int = s_ext[interior] if (interior is not None and s_ext.shape[0] == x_ext.shape[0]) else s_ext
        div_p = div_p * s_int

        # accumulate gradient
        ctx.g.add_(div_p.neg_().mul_(w))

        # energy
        E = (e_den * w).sum()
        return E if E.dtype == dr else E.to(dr)
    def add_diag(self, ctx: RegContext) -> None:
        """
        Cheap diagonal preconditioner contribution (scale-agnostic):
            diag += λ * 2 * Σ_i w_i^2
        """
        if ctx.diag is None or self.params.weight == 0.0:
            return
        axes = self._resolve_axes(ctx)
        if not axes:
            return

        if self.params.axis_weights is not None:
            if len(self.params.axis_weights) != len(axes):
                return
            sum_w2 = sum(float(w)**2 for w in self.params.axis_weights)
        else:
            sum_w2 = float(len(axes))

        k = torch.as_tensor(2.0 * self.params.weight * sum_w2, device=ctx.device, dtype=ctx.dtype_r)
        ctx.diag.add_(k)

    def continuation_update(self, stats: Mapping[str, float]) -> bool:
        return False  # policy-managed elsewhere

    def scaling_policy(self, ctx: RegContext):
        return None   # manager drives scale; kernel consumes ctx.scale_field_shard

    def prox_inplace(self, ctx: RegContext, step: float) -> None:
        pass

    def majorizer_diag(self, ctx: RegContext) -> Optional[torch.Tensor]:
        return None

    def halo(self, roles: Roles) -> Mapping[int, int]:
        """
        Request a 1-sample halo along temporal axis if it's one of the TV axes; no spatial halo here.
        """
        h = {}
        axes = self._resolve_axes_local(self.params.axes, roles)
        if any(ax == 0 for ax in axes):  # temporal is absolute axis 0 in (unlike, like, nufft)
            h[0] = 1
        return h

    # ---- internals ----
    def _resolve_axes(self, ctx: RegContext) -> Tuple[int, ...]:
        if getattr(ctx, "axes_resolver", None) is not None:
            return tuple(int(a) for a in ctx.axes_resolver(self.params.axes))
        return self._resolve_axes_local(self.params.axes, ctx.roles_image)

    @staticmethod
    def _resolve_axes_local(spec: AxesSpec, roles: Roles) -> Tuple[int, ...]:
        return _resolve_axes_local(spec, roles)

    def _axis_weights_as_tensors(self, device: torch.device, dtype: torch.dtype, n: int) -> List[torch.Tensor]:
        if self.params.axis_weights is None:
            return [torch.tensor(1.0, device=device, dtype=dtype) for _ in range(n)]
        if len(self.params.axis_weights) != n:
            raise ValueError(f"axis_weights length {len(self.params.axis_weights)} must equal number of axes {n}")
        return [torch.tensor(float(w), device=device, dtype=dtype) for w in self.params.axis_weights]

